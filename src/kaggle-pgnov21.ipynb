{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n",
    "\n",
    "Submission File\n",
    "For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:\n",
    "\n",
    "https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : kaggle-pgnov21\n",
      "    active env location : C:\\ProgramData\\Anaconda3\\envs\\kaggle-pgnov21\n",
      "            shell level : 2\n",
      "       user config file : C:\\Users\\globetrekker\\.condarc\n",
      " populated config files : C:\\Users\\globetrekker\\.condarc\n",
      "          conda version : 4.10.3\n",
      "    conda-build version : 3.21.4\n",
      "         python version : 3.8.8.final.0\n",
      "       virtual packages : __win=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : C:\\ProgramData\\Anaconda3  (writable)\n",
      "      conda av data dir : C:\\ProgramData\\Anaconda3\\etc\\conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "          package cache : C:\\ProgramData\\Anaconda3\\pkgs\n",
      "                          C:\\Users\\globetrekker\\.conda\\pkgs\n",
      "                          C:\\Users\\globetrekker\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\ProgramData\\Anaconda3\\envs\n",
      "                          C:\\Users\\globetrekker\\.conda\\envs\n",
      "                          C:\\Users\\globetrekker\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/4.10.3 requests/2.25.1 CPython/3.8.8 Windows/10 Windows/10.0.19041\n",
      "          administrator : False\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "c04be2df-43b3-4c66-ae0e-c17ff6bf7a45",
    "_uuid": "e1abf160-08a9-4188-a996-16dcee5395ec",
    "execution": {
     "iopub.execute_input": "2021-11-12T19:01:01.273778Z",
     "iopub.status.busy": "2021-11-12T19:01:01.272805Z",
     "iopub.status.idle": "2021-11-12T19:01:01.309507Z",
     "shell.execute_reply": "2021-11-12T19:01:01.308884Z",
     "shell.execute_reply.started": "2021-11-12T19:01:01.273661Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T19:01:01.311465Z",
     "iopub.status.busy": "2021-11-12T19:01:01.311078Z",
     "iopub.status.idle": "2021-11-12T19:01:03.981131Z",
     "shell.execute_reply": "2021-11-12T19:01:03.980277Z",
     "shell.execute_reply.started": "2021-11-12T19:01:01.311436Z"
    }
   },
   "outputs": [],
   "source": [
    "import time, gc, copy\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%config Completer.use_jedi = False\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T19:01:03.982595Z",
     "iopub.status.busy": "2021-11-12T19:01:03.982360Z",
     "iopub.status.idle": "2021-11-12T19:01:36.399994Z",
     "shell.execute_reply": "2021-11-12T19:01:36.399324Z",
     "shell.execute_reply.started": "2021-11-12T19:01:03.982567Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(path: str, scale: bool, debug: bool):\n",
    "    \"\"\"Import datasets from path. Expect csvs called train.csv and test.csv\n",
    "\n",
    "    Arguments:\n",
    "    :path - path containing csvs\n",
    "    :scale - run standard scaler\n",
    "    :debug - run in debug mode\n",
    "    \n",
    "    Returns:\n",
    "    :X - dataframe (train) minus target\n",
    "    :y - series (target values for train)\n",
    "    :df_test - dataframe (test) \n",
    "    \"\"\"\n",
    "    \n",
    "    if debug:\n",
    "        df_train = pd.read_csv(path + 'train.csv', nrows=1000)\n",
    "        df_test = pd.read_csv(path + 'test.csv', nrows=1000)\n",
    "    else:\n",
    "        df_train = pd.read_csv(path + 'train.csv')\n",
    "        df_test = pd.read_csv(path + 'test.csv')\n",
    "        \n",
    "    ids = df_test.id\n",
    "    df_train.drop('id', axis=1, inplace=True)\n",
    "    df_test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "    original_features = df_test.columns\n",
    "\n",
    "    X = df_train[original_features]\n",
    "    y = df_train['target']\n",
    "    \n",
    "    if scale:\n",
    "        std_scaler = StandardScaler()\n",
    "        X_norm = pd.DataFrame(std_scaler.fit_transform(X))\n",
    "        X_norm.columns = original_features\n",
    "        df_test_norm = pd.DataFrame(std_scaler.transform(df_test))\n",
    "        df_test_norm.columns = original_features\n",
    "    else:\n",
    "        X_norm = X\n",
    "        df_test_norm = df_test\n",
    "    \n",
    "    return X_norm, y, df_test_norm, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"Return list of models for initial analysis\n",
    "    \n",
    "    Returns:\n",
    "    :models - list of dicts(name, model)\n",
    "    \"\"\"\n",
    "    models = [\n",
    "        {'name': 'lr', 'model': LogisticRegression(random_state=5)},\n",
    "        {'name': 'lsvc', 'model': LinearSVC(dual=False, random_state=5)},\n",
    "        {'name': 'lgbm', 'model': LGBMClassifier(random_state=5)},\n",
    "        {'name': 'bayes', 'model': GaussianNB()},\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model, \n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series) -> list:\n",
    "    \"\"\"Return list of scores for a model\n",
    "\n",
    "    Arguments:\n",
    "    :model - model to be evaluated\n",
    "    :X - dataframe (train) minus target\n",
    "    :y - series (target values for train)\n",
    "    \n",
    "    Returns:\n",
    "    scores - list of scores for model\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_val_set(\n",
    "    model, \n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series) -> float:\n",
    "    \"\"\"Return scores for a model on validation set\n",
    "\n",
    "    Arguments:\n",
    "    :model - model to be evaluated\n",
    "    :X_train - training dataframe minus target\n",
    "    :y_train - training series (target values for training set)\n",
    "    :X_val - validation dataframe minus target\n",
    "    :y_val - validation series (target values for validation set)\n",
    "    \n",
    "    Returns:\n",
    "    score - score for model\n",
    "    \"\"\"\n",
    "    if model.__class__.__name__ == 'LinearSVC':\n",
    "        clf = CalibratedClassifierCV(base_estimator=model, cv=5)\n",
    "    else:\n",
    "        clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict_proba(X_val)[:,1]\n",
    "    \n",
    "    score = roc_auc_score(y_val, preds)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(\n",
    "    X_in: pd.DataFrame, \n",
    "    y_in: pd.Series, \n",
    "    model_type: str, \n",
    "    k: int) -> pd.DataFrame:\n",
    "    \"\"\"Return feature importances of features as to the target prediction\n",
    "\n",
    "    Arguments:\n",
    "    :X_in - dataframe (train) minus target\n",
    "    :y_in - series (target values for train)\n",
    "    :model_type - 'regression' or 'classification'\n",
    "    :k - number of folds \n",
    "    \n",
    "    Returns:\n",
    "    :featureScores - dataframe with abs correlation value sorted in asc\n",
    "    \"\"\"\n",
    "    if model_type == 'classification':\n",
    "        bestfeatures = SelectKBest(score_func=f_classif, k=k)\n",
    "    else:\n",
    "        bestfeatures = SelectKBest(score_func=f_regression, k=k)\n",
    "    \n",
    "    fit = bestfeatures.fit(X_in, y_in)\n",
    "\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X_in.columns)\n",
    "\n",
    "    # Concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score'] \n",
    "    featureScores['Abs_score'] = abs(featureScores['Score'])\n",
    "    featureScores.sort_values(by='Score', axis=0, ascending=True, inplace=True)\n",
    "    featureScores.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    plt.bar(featureScores['Specs'], featureScores['Abs_score'])\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_labels(\n",
    "    X_in: pd.DataFrame, \n",
    "    features: list,\n",
    "    n_clusters: int) -> list:\n",
    "    \"\"\"Return kmeans labels for a dataframe\n",
    "\n",
    "    Arguments:\n",
    "    :X_in - dataframe (train) minus target\n",
    "    :features - list of important features\n",
    "    :n_clusters - number of kmeans clusters\n",
    "    \n",
    "    Returns:\n",
    "    X_temp - dataframe (train) minus target plus kmeans labels\n",
    "    \"\"\"\n",
    "    X_temp = copy.deepcopy(X_in)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=3)\n",
    "    kmeans.fit(X_temp[features])\n",
    "    X_temp['cluster'] = kmeans.predict(X_temp[features])\n",
    "    \n",
    "    return X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_dist_ratios(\n",
    "    X_in: pd.DataFrame, \n",
    "    X_val_in: pd.DataFrame,\n",
    "    X_test_in: pd.DataFrame,\n",
    "    features: list,\n",
    "    n_clusters: int) -> list:\n",
    "    \"\"\"Return kmeans labels for a dataframe\n",
    "\n",
    "    Arguments:\n",
    "    :X_in - dataframe (train) minus target\n",
    "    :X_val_in - dataframe (val) minus target\n",
    "    :X_test_in - dataframe (test) minus target\n",
    "    :features - list of important features\n",
    "    :n_clusters - number of kmeans clusters\n",
    "    \n",
    "    Returns:\n",
    "    :X_temp - dataframe (train) minus target plus kmeans dist ratios\n",
    "    :X_temp_val - dataframe (val) minus target plus kmeans dist ratios\n",
    "    :X_temp_test - dataframe (test) minus target plus kmeans dist ratios\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=3)\n",
    "    X_temp = copy.deepcopy(X_in)\n",
    "    X_temp_val = copy.deepcopy(X_val_in)\n",
    "    X_temp_test = copy.deepcopy(X_test_in)\n",
    "    \n",
    "    kmeans.fit(X_temp[features])\n",
    "    cluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\n",
    "\n",
    "    cluster_distances = kmeans.transform(X_temp[features])\n",
    "    cluster_distances_val = kmeans.transform(X_temp_val[features])\n",
    "    cluster_distances_test = kmeans.transform(X_temp_test[features])\n",
    "    \n",
    "    X_temp_cluster_distances = pd.DataFrame(cluster_distances, columns=cluster_cols, index=X_temp.index)\n",
    "    X_temp_val_cluster_distances = pd.DataFrame(cluster_distances_val, columns=cluster_cols, index=X_temp_val.index)\n",
    "    X_temp_test_cluster_distances = pd.DataFrame(cluster_distances_test, columns=cluster_cols, index=X_temp_test.index)\n",
    "\n",
    "    new_cols = []\n",
    "    for i in cluster_cols:\n",
    "        for j in cluster_cols:\n",
    "            if i != j:\n",
    "                new_col_name = i + '_' + j\n",
    "                X_temp_cluster_distances[new_col_name] = X_temp_cluster_distances[i] / X_temp_cluster_distances[j]\n",
    "                X_temp_val_cluster_distances[new_col_name] = X_temp_val_cluster_distances[i] / X_temp_val_cluster_distances[j]\n",
    "                X_temp_test_cluster_distances[new_col_name] = X_temp_test_cluster_distances[i] / X_temp_test_cluster_distances[j]\n",
    "                new_cols.append(new_col_name)\n",
    "            \n",
    "    X_temp = X_temp.join(X_temp_cluster_distances[new_cols])\n",
    "    X_temp_val = X_temp_val.join(X_temp_val_cluster_distances[new_cols])\n",
    "    X_temp_test = X_temp_test.join(X_temp_test_cluster_distances[new_cols])\n",
    "    \n",
    "    return X_temp, X_temp_val, X_temp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_features_model(model, X_in, y_in, cv):\n",
    "    \"\"\"Generate meta features for single base classifier model, to be used later for stacking\n",
    "\n",
    "    Arguments:\n",
    "    :model - model to evaluate\n",
    "    :X_in - dataframe with features minus target\n",
    "    :y_in - target series\n",
    "    :cv - cross-validation iterator \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    n_classes = len(np.unique(y_in)) # Assuming that training data contains all classes\n",
    "    meta_features = np.zeros((X_in.shape[0], n_classes)) \n",
    "    n_splits = cv.get_n_splits(X_in, y_in)\n",
    "    \n",
    "    # Loop over folds\n",
    "    print(\"Starting hold out prediction with {} splits for {}.\".format(n_splits, model.__class__.__name__))\n",
    "    for train_idx, hold_out_idx in cv.split(X_in, y_in): \n",
    "        \n",
    "        # Split data\n",
    "        X_in_train = X_in.iloc[train_idx]    \n",
    "        y_in_train = y_in.iloc[train_idx]\n",
    "        X_in_hold_out = X_in.iloc[hold_out_idx]\n",
    "\n",
    "        # Fit estimator to K-1 parts and predict on hold out part\n",
    "        est = copy.deepcopy(model)\n",
    "        est.fit(X_in_train, y_in_train)\n",
    "        y_in_hold_out_pred = est.predict_proba(X_in_hold_out)\n",
    "        \n",
    "        # Fill in meta features\n",
    "        meta_features[hold_out_idx] = y_in_hold_out_pred\n",
    "\n",
    "    return meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_df(models, X_input, X_input_km, y_in):\n",
    "    # Loop over classifier to produce meta features\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
    "    meta_train = []\n",
    "    meta_test = []\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        if name == 'lr':\n",
    "            X_in = X_input\n",
    "        elif name == 'lsvc':\n",
    "            X_in = X_input\n",
    "        else:\n",
    "            X_in = X_input_km\n",
    "        \n",
    "        # Create hold out predictions for a classifier\n",
    "        if model['model'].__class__.__name__ == 'LinearSVC':\n",
    "            clf = CalibratedClassifierCV(base_estimator=model['model'], cv=5)\n",
    "        else:\n",
    "            clf = model['model']\n",
    "        meta_train_model = generate_meta_features_model(clf, X_in, y_in, cv)\n",
    "\n",
    "        # Remove redundant column - 0th column = 1-first column in a two class dataset \n",
    "        meta_train_model = np.delete(meta_train_model, 0, axis=1).ravel()\n",
    "        print(pd.DataFrame(meta_train_model).head())\n",
    "\n",
    "        # Gather meta training data\n",
    "        meta_train.append(meta_train_model)\n",
    "\n",
    "    meta_train = np.array(meta_train).T \n",
    "    df_meta_train = pd.DataFrame(meta_train)\n",
    "\n",
    "    # Optional (Add original features to meta)\n",
    "    df_meta_train = pd.DataFrame(np.concatenate((df_meta_train, X_in), axis=1))\n",
    "    \n",
    "    return df_meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_df_val(models, stack_model, X_input, X_input_km, y_in, X_test_input, X_test_km_input, features, ids):\n",
    "    \n",
    "    meta_test = []\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "    if name == 'lr':\n",
    "        X_in = X_input\n",
    "        X_test_in = X_test_input\n",
    "    elif name == 'lsvc':\n",
    "        X_in = X_input\n",
    "        X_test_in = X_test_input\n",
    "    else:\n",
    "        X_in = X_input_km\n",
    "        X_test_in = X_test_km_input\n",
    "\n",
    "    clf.fit(X_in, y_in)\n",
    "    meta_test_model = clf.predict_proba(X_test_in)\n",
    "\n",
    "    # Remove redundant column - 0th column = 1-first column in a two class dataset \n",
    "    meta_test_model = np.delete(meta_test_model, 0, axis=1).ravel()\n",
    "\n",
    "    # Gather meta training data\n",
    "    meta_test.append(meta_test_model)\n",
    "\n",
    "    meta_test = np.array(meta_test).T \n",
    "    df_meta_test = pd.DataFrame(meta_test)\n",
    "\n",
    "    # Optional (Add original features to meta)\n",
    "    df_meta_test = pd.DataFrame(np.concatenate((df_meta_test, X_test_in), axis=1))\n",
    "    \n",
    "    return df_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_logreg(trial, X_in, y_in, X_val_in, y_val_in):\n",
    "    \"\"\"Optimize logistic regression model using optuna\"\"\"\n",
    "    \n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'newton-cg', 'lbfgs', 'newton-cg', 'sag', 'saga'])\n",
    "    C = trial.suggest_float(\"C\", 0.01, 2.0)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 10000, step=100)\n",
    "    \n",
    "    penalty = 'l2'\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=max_iter, solver=solver, penalty=penalty)\n",
    "    model.fit(X_in, y_in)\n",
    "    preds = model.predict_proba(X_val_in)[:,1]\n",
    "    score = roc_auc_score(y_val_in, preds)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_linearSVC(trial, X_in, y_in, X_val_in, y_val_in):\n",
    "    \"\"\"Optimize linear SVC model using optuna\"\"\"\n",
    "    \n",
    "    C = trial.suggest_float(\"C\", 0.01, 2.0)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 1000, 10000, step=1000)\n",
    "    \n",
    "    model = LinearSVC(C=C\n",
    "                      , max_iter=max_iter\n",
    "                      , dual=False\n",
    "                      , random_state=5)\n",
    "    clf = CalibratedClassifierCV(base_estimator=model, cv=5)\n",
    "    clf.fit(X_in, y_in)\n",
    "    \n",
    "    preds = clf.predict_proba(X_val_in)[:,1]\n",
    "    score = roc_auc_score(y_val_in, preds)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_scores(models, X_in, y_in):\n",
    "    \"\"\"evaluate baseline cross val scores of the models\"\"\"\n",
    "    results, names = list(), list()\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        scores = evaluate_model(model['model'], X_in, y_in)\n",
    "        model['init_scores'] = np.mean(scores)\n",
    "        results.append(scores)\n",
    "        names.append(name)\n",
    "        print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "\n",
    "    # plot model performance for comparison\n",
    "    plt.boxplot(results, labels=names, showmeans=True)\n",
    "    plt.title('Baseline Scores')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_scores_validation(models, X_in, y_in, X_val_in, y_val_in):\n",
    "    \"\"\"evaluate baseline scores of the models on validation set\"\"\"\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        score = evaluate_model_val_set(model['model'], X_in, y_in, X_val_in, y_val_in)\n",
    "        model['init_scores_val'] = score\n",
    "        print('>%s %.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_scores(featureScores, X_in, X_val_in, X_test_in, models):\n",
    "    \"\"\"evaluate cross val scores for the models by adding kmeans cluster distance ratios\"\"\"\n",
    "    results, names = list(), list()\n",
    "    important_features = list(featureScores.sort_values(by='Abs_score', ascending=False).head(15)['Specs'])\n",
    "    X_train_km, X_val_km, X_test_km = get_kmeans_dist_ratios(X_train, X_val, df_test, important_features, 10)\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        if name == 'lr':\n",
    "            pass\n",
    "        elif name == 'lsvc':\n",
    "            pass\n",
    "        else:\n",
    "            scores = evaluate_model(model['model'], X_train_km, y_train)\n",
    "            model['kmeans_dist_rat_scores'] = np.mean(scores)\n",
    "            results.append(scores)\n",
    "            names.append(name)\n",
    "            print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "\n",
    "    # plot model performance for comparison\n",
    "    plt.boxplot(results, labels=names, showmeans=True)\n",
    "    plt.title('KMeans Distance Ratio Scores')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_scores_validation(featureScores, X_in, X_val_in, X_test_in, models):\n",
    "    \"\"\"evaluate scores for the models by adding kmeans cluster distance ratios on validation set\"\"\"\n",
    "    important_features = list(featureScores.sort_values(by='Abs_score', ascending=False).head(15)['Specs'])\n",
    "    X_train_km, X_val_km, X_test_km = get_kmeans_dist_ratios(X_in, X_val_in, X_test_in, important_features, 10)\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        if name == 'lr':\n",
    "            pass\n",
    "        elif name == 'lsvc':\n",
    "            pass\n",
    "        else:\n",
    "            score = evaluate_model_val_set(model['model'], X_train_km, y_train, X_val_km, y_val)\n",
    "            model['kmeans_dist_rat_scores_val'] = score\n",
    "            print('>%s %.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking_scores(featureScores, X_in, y_in, X_val_in, X_test_in, models, stack_model):\n",
    "    \"\"\"evaluate cross val scores with stacking\"\"\"\n",
    "    # get km enhanced df\n",
    "    important_features = list(featureScores.sort_values(by='Abs_score', ascending=False).head(15)['Specs'])\n",
    "    X_train_km, X_val_km, X_test_km = get_kmeans_dist_ratios(X_in, X_val_in, X_test_in, important_features, 10)\n",
    "    \n",
    "    # get meta df\n",
    "    df_meta_train = get_stack_df(models, X_in, X_train_km, y_in)\n",
    "\n",
    "    # cross val scores from stacking\n",
    "    results = list()\n",
    "    name = [model['name'] +'_' for model in models]\n",
    "    scores = evaluate_model(stack_model, df_meta_train, y_in)\n",
    "    score = np.mean(scores)\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binning_scores(baseline_score, X_in, y_in, model_in, improvement):\n",
    "    \"\"\"check cross val score for improvement in score by binning column\"\"\"\n",
    "    improved_cols = []\n",
    "    for col in X_in.columns:\n",
    "        print(col)\n",
    "        if model_in.__class__.__name__ == 'LinearSVC':\n",
    "            clf = CalibratedClassifierCV(base_estimator=model, cv=n_splits)\n",
    "        else:\n",
    "            clf = model\n",
    "        \n",
    "        X_new = copy.deepcopy(X_in)\n",
    "        new_col = col + '_bin'\n",
    "        X_new[new_col], bins = pd.qcut(X_in[col], q=1000, retbins=True, labels=False)\n",
    "\n",
    "        scores = evaluate_model(model, X_new, y_in)\n",
    "        new_score = scores.mean()\n",
    "        if new_score >= baseline_score + 0.00001:\n",
    "            new_col = {'col': col, 'score': new_score}\n",
    "            improved_cols.append(new_col)\n",
    "    return improved_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binning_scores_val(baseline_score, X_in, y_in, X_val_in, y_val_in, model_in, improvement):\n",
    "    \"\"\"check score in validation set for improvement in score by binning column\"\"\"\n",
    "    improved_cols = []\n",
    "    \n",
    "    for col in X_in.columns:\n",
    "        print(col)\n",
    "        if model_in.__class__.__name__ == 'LinearSVC':\n",
    "            clf = CalibratedClassifierCV(base_estimator=model, cv=n_splits)\n",
    "        else:\n",
    "            clf = model\n",
    "        \n",
    "        X_new = copy.deepcopy(X_in)\n",
    "        X_val_new = copy.deepcopy(X_val_in)\n",
    "        new_col = col + '_bin'\n",
    "        \n",
    "        X_new[new_col], bins = pd.qcut(X_in[col], q=1000, retbins=True, labels=False)\n",
    "        X_val_new[new_col] = pd.cut(X_val_new[col], bins=bins, labels=False, include_lowest=True)\n",
    "        X_val_new[new_col].fillna(X_val_new[new_col].mode()[0], inplace=True)\n",
    "        \n",
    "        score = evaluate_model_val_set(clf, X_new, y_in, X_val_new, y_val_in)\n",
    "        if score >= baseline_score + improvement:\n",
    "            new_col = {'col': col, 'score': new_score}\n",
    "            improved_cols.append(new_col)\n",
    "    return improved_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_pred_stack(models_in, stack_model_in, X_in, y_in, original_features, X_test_in, ids):\n",
    "    \"\"\"generate predictions on test df using stacking\"\"\"\n",
    "    \n",
    "    X_in = X_in[original_features]\n",
    "    X_test_in = df_test_in[original_features]\n",
    "    \n",
    "    # get km enhanced df\n",
    "    important_features = list(featureScores.sort_values(by='Abs_score', ascending=False).head(15)['Specs'])\n",
    "    X_train_km, X_val_km, X_test_km = get_kmeans_dist_ratios(X_in, X_val_in, X_test_in, important_features, 10)\n",
    "    \n",
    "    # get meta df\n",
    "    df_meta_train = get_stack_df(models, X_in, X_train_km, y_in)\n",
    "    \n",
    "    \n",
    "    meta_test_in = []\n",
    "    for model in models_in:\n",
    "        # Fit model\n",
    "\n",
    "        # Create hold out predictions for a classifier\n",
    "        if model.__class__.__name__ == 'LinearSVC':\n",
    "            clf = CalibratedClassifierCV(base_estimator=model, cv=n_splits)\n",
    "        else:\n",
    "            clf = model\n",
    "        \n",
    "        clf.fit(X_in, y_in)\n",
    "        meta_test_model = clf.predict_proba(X_test_in)\n",
    "    \n",
    "        # Remove redundant column - 0th column = 1-first column in a two class dataset \n",
    "        meta_test_model = np.delete(meta_test_model, 0, axis=1).ravel()\n",
    "    \n",
    "        # Gather meta training data\n",
    "        meta_test_in.append(meta_test_model)\n",
    "    \n",
    "        meta_test_in = np.array(meta_test_in).T \n",
    "        X_meta_test_in = pd.DataFrame(meta_test_in)\n",
    "\n",
    "    # Optional (Add original features to meta)\n",
    "    X_meta_test_in = pd.DataFrame(np.concatenate((X_meta_test_in, X_test_in), axis=1))\n",
    "    \n",
    "    stack_model_in.fit(pd.DataFrame(X_meta_train), y_in)\n",
    "\n",
    "    # Final output\n",
    "    preds = stack_model.predict_proba(X_meta_test)[:,1]\n",
    "    output = pd.DataFrame({'id': ids, 'target': preds})\n",
    "    output.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    preds = clf.predict_proba(df_test_in)[:,1]\n",
    "    df_preds = pd.DataFrame({'id': ids_in, 'target': preds})\n",
    "    return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_pred_single(model_in, X_in, y_in, X_test_in, ids):\n",
    "    \"\"\"make final test predictions using a single model\"\"\"\n",
    "    if model_in.__class__.__name__ == 'LinearSVC':\n",
    "        clf = CalibratedClassifierCV(base_estimator=model, cv=n_splits)\n",
    "    else:\n",
    "        clf = model\n",
    "        \n",
    "    clf.fit(X_in, y_in)\n",
    "    preds = clf.predict_proba(X_test_in)[:,1]\n",
    "\n",
    "    output = pd.DataFrame({'id': ids, 'target': preds})\n",
    "    output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial):\n",
    "    \"\"\"optuna objective function for lightgbm\"\"\"\n",
    "    \n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 11, 101, step=10)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 10, step=1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 2.0)\n",
    "    \n",
    "    model = LGBMClassifier(num_leaves=num_leaves\n",
    "                           , max_depth=max_depth\n",
    "                           , learning_rate=learning_rate\n",
    "                           , objective='binary'\n",
    "                           , random_state=5)\n",
    "    model.fit(X_train_norm, y_train_norm)\n",
    "    preds = model.predict_proba(X_val_norm)[:,1]\n",
    "    score = roc_auc_score(y_val_norm, preds)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_linear_svc(X_in, y_in, X_val_in, y_val_in, n_trials):\n",
    "    \"\"\"optimize linear svc using optuna\"\"\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=\"LinearSVC\")\n",
    "    func = lambda trial: objective_linearSVC(trial, X_in, y_in,  X_val_in, y_val_in)\n",
    "    study.optimize(func, n_trials=n_trials)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "    print('Best score:', study.best_value)\n",
    "    \n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_logreg(X_in, y_in, X_val_in, y_val_in, n_trials):\n",
    "    \"\"\"optimize logistic regression using optuna\"\"\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=\"Logistic regression\")\n",
    "    func = lambda trial: objective_logreg(trial, X_train, y_train,  X_val, y_val)\n",
    "    study.optimize(func, n_trials=n_trials)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "    print('Best score:', study.best_value)\n",
    "    \n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm(X_in, y_in, X_val_in, y_val_in, n_trials):\n",
    "    \"\"\"optimize lightgbm using optuna\"\"\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=\"LightGBM\")\n",
    "    func = lambda trial: objective_lightgbm(trial)\n",
    "    study.optimize(func, n_trials=n_trials)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "    print('Best score:', study.best_value)\n",
    "    \n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "X, y, df_test, ids = get_datasets('../input/tabular-playground-series-nov-2021/', True, False)\n",
    "original_features = list(df_test.columns)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, train_size=0.8)\n",
    "\n",
    "# get models\n",
    "models = get_models()\n",
    "\n",
    "# get baseline scores\n",
    "get_baseline_scores(models, X_train, y_train)\n",
    "\n",
    "# get baseline scores on validation set\n",
    "get_baseline_scores_validation(models, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# get feature importances\n",
    "featureScores = get_feature_importances(X_train, y_train, 'classification', 5)\n",
    "\n",
    "# get kmeans scores\n",
    "get_kmeans_scores(featureScores, X_train, X_val, df_test, models)\n",
    "\n",
    "# get kmeans scores on validation set\n",
    "get_kmeans_scores_validation(featureScores, X_train, X_val, df_test, models)\n",
    "\n",
    "# get stacking scores\n",
    "stack_model = LogisticRegression(solver='sag', C=1.6213309780417264, max_iter=1800, random_state=10)\n",
    "get_stacking_scores(featureScores, X_train, y_train, X_val, df_test, models, stack_model)\n",
    "\n",
    "# get stacking scores - pop 0\n",
    "models_stack = copy.deepcopy(models)\n",
    "models_stack.pop(0)\n",
    "stack_model = LogisticRegression(solver='sag', C=1.6213309780417264, max_iter=1800, random_state=10)\n",
    "get_stacking_scores(featureScores, X_train, y_train, X_val, df_test, models_stack, stack_model)\n",
    "\n",
    "# get binning scores\n",
    "baseline_score = 0.749\n",
    "model = LinearSVC(dual=False, C=0.012249147757314706, max_iter=10000)\n",
    "improved_cols = get_binning_scores(baseline_score, X_train, y_train, model, 0.01)\n",
    "\n",
    "# get binning scores on validation set\n",
    "baseline_score = 0.749\n",
    "model = LinearSVC(dual=False, C=0.012249147757314706, max_iter=10000)\n",
    "improved_cols = get_binning_scores_val(baseline_score, X_train, y_train, X_val, y_val, model, improvement)\n",
    "\n",
    "# optimize model\n",
    "optimize_linear_svc(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# final prediction - single model - logistic regression\n",
    "clf = LogisticRegression(solver='newton-cg', C=0.023672809391721117, max_iter=200)\n",
    "make_final_pred_single(clf, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
